{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "mount_file_id": "1NpV9LMWjQNLfyXxoqV8GVPtRS5glIFDU",
      "authorship_tag": "ABX9TyM+rYgn4pil3IWMXZCV3Wlj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/DNA/blob/main/data_finetuning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ennBnx2PKi05",
        "outputId": "5a93d1f6-7786-4f58-9681-ba1eb238f0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, datasets, bitsandbytes\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 datasets-4.4.1 pyarrow-22.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes peft accelerate transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 압축풀기\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/DNA/open.zip\"\n",
        "extract_dir = \"/content/open\"\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"✅ 압축 해제 완료!\")\n",
        "print(\"압축 풀린 파일 목록:\", os.listdir(extract_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83AS8q-qb15l",
        "outputId": "a54a5c10-ea25-4d1a-890b-91649dd2984e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 압축 해제 완료!\n",
            "압축 풀린 파일 목록: ['test.csv', 'sample_submission.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJKQnE7QLe5h",
        "outputId": "07279f71-8b54-4766-d7a8-085c634feb0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = \"/content/open\"\n",
        "\n",
        "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "sub_df  = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n",
        "\n",
        "print(\"✅ test_df:\", test_df.shape)\n",
        "print(\"✅ sub_df:\", sub_df.shape)\n",
        "test_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "-rMTelo3LNWE",
        "outputId": "3de9b073-1a60-4c2b-9c0f-b15927d8ac5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test_df: (13711, 2)\n",
            "✅ sub_df: (13711, 769)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ID                                                seq\n",
              "0  TEST_000000  ATCATTTTTATTTTTTAGTTTTATGAGACGCTGCCTTGCTATGTCA...\n",
              "1  TEST_000001  CGACGTCCCCGTAGCGGCCGAAGTCGAGGGGCAGCAGGCGATCGTG...\n",
              "2  TEST_000002  GGTAGTAAGAAGGAAAATGACAGCATGGAAGCAGCAATACCAGTAA...\n",
              "3  TEST_000003  CAGCGCATATACTCAGGGCCATGGTGGGTACTGTTCCCATGGCCAG...\n",
              "4  TEST_000004  TTCATAATTGCTATCAGTCTATGGGCTAATATTTTATACATCAATG..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe489c2f-ee57-4eb7-92fd-a5e04ca5fb8e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>ATCATTTTTATTTTTTAGTTTTATGAGACGCTGCCTTGCTATGTCA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>CGACGTCCCCGTAGCGGCCGAAGTCGAGGGGCAGCAGGCGATCGTG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>GGTAGTAAGAAGGAAAATGACAGCATGGAAGCAGCAATACCAGTAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>CAGCGCATATACTCAGGGCCATGGTGGGTACTGTTCCCATGGCCAG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>TTCATAATTGCTATCAGTCTATGGGCTAATATTTTATACATCAATG...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe489c2f-ee57-4eb7-92fd-a5e04ca5fb8e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe489c2f-ee57-4eb7-92fd-a5e04ca5fb8e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe489c2f-ee57-4eb7-92fd-a5e04ca5fb8e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2f862b21-61b5-4043-8234-775ada1b0afa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2f862b21-61b5-4043-8234-775ada1b0afa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2f862b21-61b5-4043-8234-775ada1b0afa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 13711,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13711,\n        \"samples\": [\n          \"TEST_007592\",\n          \"TEST_012890\",\n          \"TEST_013273\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seq\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13711,\n        \"samples\": [\n          \"ATTTATTAAACAAATGATTAAAAAGCCATATAGATGATTTTAAGATAGCTTTTGTAAGCGGAAGCTATCTTAAAAATTAATGTTATTTACAATGTATTATCAGGTAATAATGTAAATGAATCTCCCACCAACACAAATATACCTAATCAAAGAGTAATTTTTTGTCTTCATTTTTTTCCCACATATTTTAGACTGTGTACGGAAGTTCAAGTGTTGTCAGATAAGCATAGAAGAAGGCAAAGGGAAACTCTGGTGGAATTTGAGGAAAACATGCTATAAGATAGTGGAGCACAATTGGTTCGAAACCTTCATTGTCTTCATGATTCTGCTGAGCAGTGGGGCTCTGGTAGGTGATGCATGATCCACTCCTTCACCTTTCATCTGAAATCTTTTCCCTTTCCCTTCAATCAACTCATATTACCCACTTTTAAATTAAGGTGTTTGTAAGAATGAGAAGAAATATGTGTGACGTGTTTAGCACATATGAGAGGCTTAGTAAATAGCAATTTTTATCACTCTGTCTGGAGTAGCCCTCGGGTGGAACCAAACTCAGATCATTATGGTTTCTTATAATGTTTAAAGAAGGATCTTTCTGACTTTCAGTCATCAGAGGCAGTTCTTATTAAGACTGGTTATGTAGACATGATGTAGGATTATCAGCTAAATATCAGACTGAAGCACGATATTTCCCTGACCCCTTTGCAGGTGAGAACTAGAGTGCATGGGTGCCGGTAGGAGCGAACTCCACTCACTCACTGCTCCACCCCTCACAGGAGGGGGAGCGCAGGTGACTGGGTGCAGGAGCCAAGGCAAATGCATTTGGGCACTGCAAGAGTGAACTCCATACCGGCCCCACAGGAGCGTCTAGGGGAGGGTGCCTGCGATCCTTGAAGCCCTAGAGGAAGTGTTACAGTGCCCTTTTAGCTTTGCCATCCATGGATGGCTTAAATGTTAACAGTTCAGTGGAGGGTCAGAGTGACAGCCTTTTGCACCCACACTTGTGGTACCCAAGTTCATGTCCGGC\",\n          \"CCTTTCCACCCCCATGTTGAAAATATTTGACCAATTATTTAACAGAAGGAAATAAGTAGCAGTAAGGACCACACTAATCCCAGACGGAAAATTTATAGACATGCATCATTTTTCTCTTTAAAGGCAAGTTTTGGTTAATTCCTAGAAGGACAATCTGAACGTGCAGAATTTATTAGAACACGGGAAAGTCGCAGAGTCCTCGTCCTCCAATGCACGGTCTTATCATCCCAGGCCCTCAGAACCAGCGTGCTCAAAACTCGGTCCTTTTGTGTGCACAGCTGGTTTCTCTCAGTAAAGCGCGCACACACTCGCACACACGCGCGCGCACAAAACTTCAGCCCCAACTTTGGTCGGCTTTAGGGTCCCATCCCTGAGGCAGCCTGTTTCAGCCCGCGGTCAGTACTCACGCTTGCTTTGACTGACAGCCACCGGGGACTGGGGCTGAACCATTTGCTGAGCCTGCCTCTTGCTCTTCGAGGCTCCCGTGGAGGGCACCGCTGTCCCCAGGCACTGGACGGCCAGCAGCAGCAGCCCGGGCCCCGGACCCCTAAGCATGTTGAGACGGTGGGGGAGAGACGCCCGCACCGGGAGGCAAGTTGCCACCAAGTTTGCTTCCCTTCGCAACCTGCGGGAAAAATCCCTTCTAATGCCTCCCGGGGGTTGTCGCCTCCAAGAAGGTGGGGGCCAGAGGGTGGGGAAGGGGACGGGTGGAGGGACAGAAGGGATGCAGAGGACCAGAGAAGTTGTGGCTGCAGGTCCCCTCTTCCCGCTCGCGCCTGGGGTTCCCTCTCCTCCCCCTGTGCAGCACAGCCGGCGCGGGCGTCCGAGCGCCGGGAGCCGGGGCTTATATGGGACGGTCCCCTCCCGCCCCTCTGGAGGCCCGGGGCGCGGGGGAGGAGAGACCCGCCGGGCTGTCCCCGCCCCGCCCCACCCCACCCGCCCGCCCGCCCGCCGGCCGCCGCAGCCGACCGCGCGCCGATTGGCCCGGGCTCCGGGTGACGTCACGGGGGACTGTGGGTTCGCA\",\n          \"ATTTTTAGTTGAGACAGGGTTTCTCCATGTTGGTCAGGCTGGTCTCAAACTCCCGACCTCAGGTGATCCACCGGTCTTGGCCTCCCAAAGTGTTGGGGTTACAGGCGTGAGCCACTGCGCCCGGCCTCTGATAGGAAATTCTTTAGTCTGGCATTGCATCAGAGTGAAATCTTTTTGTCAAAACACCTGAGGGTCCAGCTAGGCAGTCAGCAAATGTAGTCTGAGGGTCACTCTTTCCCTGTAAGCCAGCACTGCTTTACCTGAAAGCCCTGTAGAGAACCTCTTGCATGTGACTTAAGCTATTTAGAAAATCCAGCTAGTAGATTATTTAAGAGTTCGATACGTTAAGTAGCATTAATGACTCCTTTTTATCAAGATTGCTCTACAAAATACAGTCTGTAAATATGACAAATGACTGTATGACACAAAACAGAATTAAAATGCCCCATTAGAAATAAAATGGGCAGTAGTAAAATGAATGAGGCAGGGGAAATTTATGAGGGTAATTGATCTCTCGGTCATTAAGCAGGCTCTCTATTGGCATTTAACAAAAATCCATTTCAGGTACGTGCAGCCAGCAACAACCTCCACCCCCATTACAAAATGTTCATCCAGAGAGAGCCAAAATAACTGAGCAAATAATGAAAATAACAGTGGAAATTACCCTGTTGTACTGTTAACATAACGTACTCAGAAATACCAAGGACACAGTGATTTTCATGCCAGCTATCCAAATAGATCTTAGTAAACTTTCAGCAACTGCACTAACACAGCTTCTGGAATTTATGATGGAGTTTTCTTGTTTTAAGGTGCCATTATTCATGGAAGGAGTAGAAGAGTACCTGCTAATAATTTTGCACTGCTAAGTCCCAGGCTATACCTGTGGCAGGATTAATATCCCCAGTTTAGGATAATAAAATCAAAACTCTATGATTCACCCTTTAATAAGAGAACTTCAGGTTCAGCTCCAGGCTGCTTACCATTTTCTGCTTTGAATAGATGCCTTTTTAAAAAGTGATTTCCT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, zipfile\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 재현성\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)\n",
        "\n",
        "# 환경 변수\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"✅ Device:\", DEVICE)\n",
        "\n",
        "# 압축 해제\n",
        "def extract_open_zip():\n",
        "    candidates = [\"/content/drive/MyDrive/DNA/open.zip\", \"/content/open.zip\", \"/open.zip\"]\n",
        "    for c in candidates:\n",
        "        if os.path.exists(c):\n",
        "            zip_path = c; break\n",
        "    else:\n",
        "        raise FileNotFoundError(\"open.zip 경로를 찾을 수 없습니다.\")\n",
        "    extract_dir = \"/content/open\"\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "    print(\"✅ 압축 해제 완료:\", os.listdir(extract_dir))\n",
        "    return extract_dir\n",
        "\n",
        "data_dir = extract_open_zip()\n",
        "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "sub_df  = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n",
        "print(\"✅ test_df:\", test_df.shape)\n",
        "display(test_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "GHE5sUTELS-s",
        "outputId": "a4fedccc-c4fb-4099-a4e4-06c681529ddc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Device: cuda\n",
            "✅ 압축 해제 완료: ['test.csv', 'sample_submission.csv']\n",
            "✅ test_df: (13711, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            ID                                                seq\n",
              "0  TEST_000000  ATCATTTTTATTTTTTAGTTTTATGAGACGCTGCCTTGCTATGTCA...\n",
              "1  TEST_000001  CGACGTCCCCGTAGCGGCCGAAGTCGAGGGGCAGCAGGCGATCGTG...\n",
              "2  TEST_000002  GGTAGTAAGAAGGAAAATGACAGCATGGAAGCAGCAATACCAGTAA...\n",
              "3  TEST_000003  CAGCGCATATACTCAGGGCCATGGTGGGTACTGTTCCCATGGCCAG...\n",
              "4  TEST_000004  TTCATAATTGCTATCAGTCTATGGGCTAATATTTTATACATCAATG..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fb568ad-79ff-48af-9b61-c0c7d60a7aa5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>ATCATTTTTATTTTTTAGTTTTATGAGACGCTGCCTTGCTATGTCA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>CGACGTCCCCGTAGCGGCCGAAGTCGAGGGGCAGCAGGCGATCGTG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>GGTAGTAAGAAGGAAAATGACAGCATGGAAGCAGCAATACCAGTAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>CAGCGCATATACTCAGGGCCATGGTGGGTACTGTTCCCATGGCCAG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>TTCATAATTGCTATCAGTCTATGGGCTAATATTTTATACATCAATG...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fb568ad-79ff-48af-9b61-c0c7d60a7aa5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4fb568ad-79ff-48af-9b61-c0c7d60a7aa5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4fb568ad-79ff-48af-9b61-c0c7d60a7aa5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-11936387-2ca6-403e-8c4b-67089cd3c262\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11936387-2ca6-403e-8c4b-67089cd3c262')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-11936387-2ca6-403e-8c4b-67089cd3c262 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"TEST_000001\",\n          \"TEST_000004\",\n          \"TEST_000002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seq\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"CGACGTCCCCGTAGCGGCCGAAGTCGAGGGGCAGCAGGCGATCGTGGCTGAGCCGGATGAGGAGCTGCCCTGCGAGCTGGGCCACGGCCTGGGCCACGGCGGGCAGGCGGCCTTGCAGCACCTTATGCAGGTTCTCATAAGTGTCCTCCTTTGTGTGCAGGAATGGGTAGGCCTGGTCGTCCTGCCAGGACAGGGTGGACGCTGGGGCGGAGGCAGACAGGCTGAAGGACTAGCGCTGTGTCTGGGGCAGGGGGAGGACGTCTCACCTCCATAAAGGAGAACTCGACGGCAGGGACTCCCACAAAGGCCGTGAAGGAATAGGCACTGCTGTCCATGGGTAGGGGCCGGATCCTGGGGGCAGGTGGGTTGGAGCGATCTGTGGGTTCAGGCTCCCCCACATCCCTCCCCAGCTCTCCCTACCCCCCCAACTTACACCTCAGCATCCCAGCTGGGATTGGTGAACACCACCTGTTCATAGAGAGTCTGCCCACTGTGGTTGGGAGAATCCACCCGGGGGTTGGGGAAGGGCACTGATCAGGCTCTGCTCCTCCCCGCAACCTACTCCCCTTCACCCCCTATTCCTGGGGTGCTCTTGCCTGCTTCAGGACACTCTCAATGAGACTTGTCAGAAGGGGGCTGGTCTTGGCATGAAACTTGTCATCCCCTGGAAAAAGGGGAGGGGAGGGATGCCAGGCTCAGGGCTCAGCCCCAGAGGCAAGGGTGGACCCCAGGGGGCCAGGTGGTGGCACTGCCCCAGCTCACCCAGCACTGCGTTGTCCAGGCTCACGTACACTACGGCTTTGAGGTGCAGCACGCTGAGGTAGCCCTGTGGGTGGGTGACCAGTGTGGAGTGGGAACCCCCCACCCCCACCTCTCCCCAATGCCTAACGACCTGCCCGGGACCCCGTATCACCTCTAGCCACTCCGTGGAGCCCACGCTTCCAAAGTCACCACCGTCCCAGCTGATGAAGAGGAGACTTCTGCGGGGCCGGAAGCCTGGGGACAGAGGGGAAGGAGGACAGGC\",\n          \"TTCATAATTGCTATCAGTCTATGGGCTAATATTTTATACATCAATGGAAACTGGTCAAAACCAAAATTATAAAATTAAATGTCACCTGCGTAAAATATAGAGACAAGGTATAAAATAGAAATTAAATATGTGGCCGGGCGCGGTGGCTCACGTCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGATCACGAGGTCAGGAGATCGAGACCATCCTGGATAACACGGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCCGGGCGAGGTGGCGGTCGCCTGTAGTCCCAGCTACGCGGGAGGCTGAGGCAGGAGAATGGCGTGAACCCGGGAGGCAGAGCTTGCAGTGAGCCGAGATCGCGCCACTGCACTCCAGCCTGGGCTACAGCGAGACTCCGCCTCAAAAAAAAAAAAAAAAAAAAAAAAAATTAAATATGTATGTTCATTATTGGAAGAACCCCGCTAACTGGCTTTGACAGTAGAGCATGGTGGTTAATGCACTCTGAAATCAGCCCAGACAAGATCAGTCCCTATTCTACTCCAAACACGGTGTGAATTTACACAAATCATGTCACCTCTCTGTGCTTGAGTCCTCTTTGTCTTTATAATGGTGGTACCTGGCTCATAGATTATGAAGAATATGGAGGTGAATACATGTCTGTTCTTCATAGCAAAGAGCAATTCATTGCAAATACACTTAAAATGGGACACATCCAAATATGGCAAACTCTTCATACTAGAAAGTAACTTTATTAAGTATTTCTATTGCATTTCAGCTGACTTTATTTTTTATTTTTATTTTTTATGTTTTGAGACAGGGTCTCACTTTGTCATCCAGCCTGGTGTGCAGTGGCGGGATCTCGGCTCGCTGCAGCCTCAATCTCCTGGGCTCAAGGGATCCTCCTACCTCAGCCCCCACCAAGTACCAGCTGCCTTTTAAATGTCGCTCATGCATCACGTTCCCTGCAGGATCCTGTGTCTCTGAAGATAAGTAGCAGCCTGTGGGAACTGACAGA\",\n          \"GGTAGTAAGAAGGAAAATGACAGCATGGAAGCAGCAATACCAGTAATAGTAGCCTCTGTGACACCAGGGCGGGGCCGAGGGACCACTTCTCTGGGAGGAGACCCAGGCTTCTCATACTTGATGATGTAGCCGGTAATCCTGGCACGTGGCGGCTGCCATGATACCAGCAAGGAATTGGGTGTGGTGGCCAGGAAACGCAGGTTGGATGGTGCATCAATGGCTGAAAGAAAACAAAATTAAGTCTCTAAGAAGGCAAATAACCAAGAAAATTACTCCCACACTTTTCTCCCGAGCCGTTCTAAACACACTTAAAAGAATCTGAGAATACTGTAAACCGGAAACAAGATTCTCTGGAATCTATAATACAAAAATATACGCCTCACATTTGTTTTTTGAGTTCATGAAACTGATTGCATACAAGTCAATGGCATTTCCTCAGTAGAAGGTATAGTTACCAGTGGAGGCGTCGATGACCACAGGGGAGCTCCGAGCATTGTCATTCAAGGTGTACAGGTAGATCTTGTAGTCAGTGCCTGGTTGTAAACCTGGGATTTGAGAAGAGATGATTTTTAACAGTTCTTGCTTTTTACTAGGAGAATTCTCAAGCTAGCCTGCAATTTTTCTGGGTTCTTCTATCATCCCATATTTATATAGACTACCAAAGTGGTCCACATTTGAACAGTAAAGTAGAGCATGCTATGGATGTTAACTGTTAACAAGAAACATATCTCGAAAGTGAATCACTGATACAGACAAGGACACTGGTAAAGTAAATTCTGGAATGCAAAAATGCTATGTTGGAAGAGGCCCTCTTGCAGAATGGGGGTCAGCCACTTATTGAGGTTTTCTCTTTACCGTAATCAGATGGTCAATAGTTGAGCCTTGGAATCACAATTTTAAAAAGCTAAAAAAGACCTCAGATATTGAATATTCTTCGGTTACAATTTTGGCCTTTATTAAGTGGTATTGTTTACTTTTATATTAACTATGCTTTTTAATGCTAAGATCTATACATCACTGTTTT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gc_content(seq: str) -> float:\n",
        "    seq = seq.upper()\n",
        "    return (seq.count(\"G\") + seq.count(\"C\")) / len(seq) if len(seq) > 0 else 0.0\n",
        "\n",
        "def seq_entropy(seq: str) -> float:\n",
        "    c = Counter(seq)\n",
        "    n = len(seq)\n",
        "    return 0.0 if n == 0 else -sum((v/n)*math.log2(v/n) for v in c.values())\n",
        "\n",
        "corpus_df = test_df.copy()\n",
        "corpus_df[\"gc\"] = corpus_df[\"seq\"].apply(gc_content)\n",
        "corpus_df[\"entropy\"] = corpus_df[\"seq\"].apply(seq_entropy)\n",
        "ENT_MIN, ENT_MAX = corpus_df[\"entropy\"].min(), corpus_df[\"entropy\"].max()\n",
        "print(\"✅ 파생변수 생성 완료\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8tZeWvOre0",
        "outputId": "7eee4465-d79e-4a8c-fa5e-1caf4b6c34e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 파생변수 생성 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
        "\n",
        "# 8-bit 로딩\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# ✅ MLM용: FEATURE_EXTRACTION 사용\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"query\",\"key\",\"value\",\"dense\",          # attention\n",
        "        \"intermediate.dense\",\"output.dense\"     # FFN (있으면 자동 적용, 없으면 무시)\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.FEATURE_EXTRACTION,      # ← 여기!\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwBpk5iBMdBB",
        "outputId": "2c70ea2c-b25a-43e6-f6e6-924fad55229c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 5,242,880 || all params: 503,588,316 || trainable%: 1.0411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1개 시퀀스로 더미 전/후 처리 테스트\n",
        "import torch\n",
        "tmp = tokenizer(\"ACGT\"*256, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "with torch.no_grad():\n",
        "    _ = model(**{k: v.to(model.device) for k,v in tmp.items()})\n",
        "print(\"✅ forward ok\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGyraSCBeS0J",
        "outputId": "34fe96c8-2728-4763-f54d-d8023c26d76f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ forward ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "class MLMDataset(Dataset):\n",
        "    def __init__(self, df): self.df = df\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        x = self.df.iloc[i]\n",
        "        return {\"seq\": x.seq, \"gc\": x.gc, \"entropy\": x.entropy}\n",
        "\n",
        "def mask_prob_from_gc_entropy(gc, entropy, base=0.15, gc_center=0.5, gc_w=0.10, ent_w=0.05):\n",
        "    gc_factor = 1.0 - abs(gc - gc_center)\n",
        "    ent_norm = (entropy - ENT_MIN) / (ENT_MAX - ENT_MIN + 1e-8)\n",
        "    return float(np.clip(base + gc_w*gc_factor + ent_w*ent_norm, 0.05, 0.40))\n",
        "\n",
        "class VariantAwareMLMCollator:\n",
        "    def __init__(self, tok, max_len=512):\n",
        "        self.tok, self.max_len = tok, max_len\n",
        "        self.pad_id, self.mask_id = tok.pad_token_id, tok.mask_token_id\n",
        "        self.vocab_size = tok.vocab_size\n",
        "        self.special_ids = set(tok.all_special_ids)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        seqs = [b[\"seq\"] for b in batch]\n",
        "        gcs  = [b[\"gc\"] for b in batch]\n",
        "        ents = [b[\"entropy\"] for b in batch]\n",
        "        enc = self.tok.batch_encode_plus(seqs, padding=\"longest\", truncation=True,\n",
        "                                         max_length=self.max_len, return_tensors=\"pt\")\n",
        "        input_ids, attn = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
        "        labels = input_ids.clone()\n",
        "        B, L = input_ids.shape\n",
        "        rand = torch.rand((B, L))\n",
        "        probs = torch.tensor([mask_prob_from_gc_entropy(gc, ent) for gc, ent in zip(gcs, ents)]).unsqueeze(1)\n",
        "        mask_cand = (rand < probs) & (input_ids != self.pad_id)\n",
        "        for sid in self.special_ids: mask_cand &= (input_ids != sid)\n",
        "        labels[~mask_cand] = -100\n",
        "        mask80 = mask_cand & (torch.rand((B, L)) < 0.8)\n",
        "        input_ids[mask80] = self.mask_id\n",
        "        mask10 = mask_cand & (~mask80) & (torch.rand((B, L)) < 0.5)\n",
        "        rand_tokens = torch.randint(self.vocab_size, (B, L))\n",
        "        input_ids[mask10] = rand_tokens[mask10]\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
        "\n",
        "# Dataloader\n",
        "mlm_loader = DataLoader(MLMDataset(corpus_df), batch_size=4, shuffle=True,\n",
        "                        collate_fn=VariantAwareMLMCollator(tokenizer, max_len=512))\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=100, num_training_steps=len(mlm_loader))\n",
        "scaler = GradScaler(\"cuda\")\n",
        "\n",
        "ACCUM = 64\n",
        "model.train()\n",
        "for step, batch in enumerate(mlm_loader, 1):\n",
        "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "        out = model(**batch)\n",
        "        loss = out.loss / ACCUM\n",
        "    scaler.scale(loss).backward()\n",
        "    if step % ACCUM == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optim); scaler.update()\n",
        "        optim.zero_grad(set_to_none=True); scheduler.step()\n",
        "    if step % 100 == 0:\n",
        "        print(f\"[S1] Step {step}/{len(mlm_loader)} Loss: {loss.item()*ACCUM:.4f}\")\n",
        "\n",
        "stage1_ckpt = \"/content/stage1_lora8bit\"\n",
        "os.makedirs(stage1_ckpt, exist_ok=True)\n",
        "model.save_pretrained(stage1_ckpt); tokenizer.save_pretrained(stage1_ckpt)\n",
        "print(\"✅ Stage 1 저장 완료:\", stage1_ckpt)\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBRl0XSWcTPn",
        "outputId": "70744599-3c14-436b-a31b-f01f1d769a72"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S1] Step 100/3428 Loss: 7.4927\n",
            "[S1] Step 200/3428 Loss: 8.5928\n",
            "[S1] Step 300/3428 Loss: 6.8962\n",
            "[S1] Step 400/3428 Loss: 7.2877\n",
            "[S1] Step 500/3428 Loss: 8.0956\n",
            "[S1] Step 600/3428 Loss: 8.2488\n",
            "[S1] Step 700/3428 Loss: 6.0480\n",
            "[S1] Step 800/3428 Loss: 7.6088\n",
            "[S1] Step 900/3428 Loss: 6.5175\n",
            "[S1] Step 1000/3428 Loss: 6.5289\n",
            "[S1] Step 1100/3428 Loss: 7.0043\n",
            "[S1] Step 1200/3428 Loss: 6.5666\n",
            "[S1] Step 1300/3428 Loss: 6.8615\n",
            "[S1] Step 1400/3428 Loss: 6.3027\n",
            "[S1] Step 1500/3428 Loss: 6.5134\n",
            "[S1] Step 1600/3428 Loss: 7.7776\n",
            "[S1] Step 1700/3428 Loss: 6.2032\n",
            "[S1] Step 1800/3428 Loss: 7.4267\n",
            "[S1] Step 1900/3428 Loss: 5.4820\n",
            "[S1] Step 2000/3428 Loss: 7.6738\n",
            "[S1] Step 2100/3428 Loss: 6.8451\n",
            "[S1] Step 2200/3428 Loss: 7.0835\n",
            "[S1] Step 2300/3428 Loss: 7.3865\n",
            "[S1] Step 2400/3428 Loss: 6.7172\n",
            "[S1] Step 2500/3428 Loss: 7.6374\n",
            "[S1] Step 2600/3428 Loss: 7.0682\n",
            "[S1] Step 2700/3428 Loss: 8.3708\n",
            "[S1] Step 2800/3428 Loss: 6.5918\n",
            "[S1] Step 2900/3428 Loss: 6.8777\n",
            "[S1] Step 3000/3428 Loss: 7.6026\n",
            "[S1] Step 3100/3428 Loss: 6.5234\n",
            "[S1] Step 3200/3428 Loss: 6.6713\n",
            "[S1] Step 3300/3428 Loss: 6.1197\n",
            "[S1] Step 3400/3428 Loss: 8.0215\n",
            "✅ Stage 1 저장 완료: /content/stage1_lora8bit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASES = [\"A\",\"C\",\"G\",\"T\"]\n",
        "def random_snv(seq, num_mut=1):\n",
        "    s = list(seq)\n",
        "    for pos in random.sample(range(len(s)), min(num_mut, len(s))):\n",
        "        cand = [b for b in BASES if b != s[pos]]\n",
        "        s[pos] = random.choice(cand)\n",
        "    return \"\".join(s)\n",
        "\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, df): self.df = df\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        seq = self.df.iloc[i].seq\n",
        "        return {\"seq\": seq, \"seq_view\": seq, \"seq_mut\": random_snv(seq, 1)}\n",
        "\n",
        "def collate_contrastive(batch, tok=tokenizer, max_len=512):\n",
        "    def enc(xs):\n",
        "        e = tok.batch_encode_plus(xs, padding=\"longest\", truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "        return e[\"input_ids\"], e[\"attention_mask\"]\n",
        "    seqs = [b[\"seq\"] for b in batch]\n",
        "    seqs_view = [b[\"seq_view\"] for b in batch]\n",
        "    seqs_mut = [b[\"seq_mut\"] for b in batch]\n",
        "    oi, om = enc(seqs); vi, vm = enc(seqs_view); mi, mm = enc(seqs_mut)\n",
        "    return {\"oi\":oi,\"om\":om,\"vi\":vi,\"vm\":vm,\"mi\":mi,\"mm\":mm}\n",
        "\n",
        "from torch.nn.functional import cosine_similarity\n",
        "def mean_pool_last_hidden(outputs, mask):\n",
        "    hs = outputs.hidden_states[-1]\n",
        "    m = mask.unsqueeze(-1)\n",
        "    return (hs*m).sum(1)/m.sum(1).clamp(min=1)\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(stage1_ckpt, trust_remote_code=True,\n",
        "                                             quantization_config=bnb_config, device_map={\"\":0})\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.train()\n",
        "\n",
        "ctr_loader = DataLoader(ContrastiveDataset(corpus_df), batch_size=4, shuffle=True,\n",
        "                        collate_fn=lambda b: collate_contrastive(b, tok=tokenizer))\n",
        "\n",
        "optim2 = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "scaler2 = GradScaler(\"cuda\")\n",
        "ACCUM2, MARGIN = 64, 0.1\n",
        "\n",
        "for step, batch in enumerate(ctr_loader, 1):\n",
        "    oi, om = batch[\"oi\"].to(DEVICE), batch[\"om\"].to(DEVICE)\n",
        "    vi, vm = batch[\"vi\"].to(DEVICE), batch[\"vm\"].to(DEVICE)\n",
        "    mi, mm = batch[\"mi\"].to(DEVICE), batch[\"mm\"].to(DEVICE)\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "        zo = mean_pool_last_hidden(model(oi, attention_mask=om, output_hidden_states=True), om)\n",
        "        zv = mean_pool_last_hidden(model(vi, attention_mask=vm, output_hidden_states=True), vm).detach()\n",
        "        zm = mean_pool_last_hidden(model(mi, attention_mask=mm, output_hidden_states=True), mm).detach()\n",
        "        sim_pos = cosine_similarity(zo, zv)\n",
        "        sim_neg = cosine_similarity(zo, zm)\n",
        "        loss = torch.clamp(MARGIN + sim_neg - sim_pos, min=0).mean() / ACCUM2\n",
        "    scaler2.scale(loss).backward()\n",
        "    if step % ACCUM2 == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler2.step(optim2); scaler2.update(); optim2.zero_grad(set_to_none=True)\n",
        "    if step % 100 == 0:\n",
        "        print(f\"[S2] Step {step} loss: {loss.item()*ACCUM2:.4f}\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "stage2_ckpt = \"/content/stage2_contrastive_lora8bit\"\n",
        "os.makedirs(stage2_ckpt, exist_ok=True)\n",
        "model.save_pretrained(stage2_ckpt); tokenizer.save_pretrained(stage2_ckpt)\n",
        "print(\"✅ Stage 2 저장 완료:\", stage2_ckpt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlMidBFWed2C",
        "outputId": "0bf73063-9751-4697-cdc7-090e90dd5ff0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S2] Step 100 loss: 0.0998\n",
            "[S2] Step 200 loss: 0.0997\n",
            "[S2] Step 300 loss: 0.0998\n",
            "[S2] Step 400 loss: 0.0994\n",
            "[S2] Step 500 loss: 0.0990\n",
            "[S2] Step 600 loss: 0.0988\n",
            "[S2] Step 700 loss: 0.0995\n",
            "[S2] Step 800 loss: 0.0991\n",
            "[S2] Step 900 loss: 0.0972\n",
            "[S2] Step 1000 loss: 0.0992\n",
            "[S2] Step 1100 loss: 0.0994\n",
            "[S2] Step 1200 loss: 0.0998\n",
            "[S2] Step 1300 loss: 0.0974\n",
            "[S2] Step 1400 loss: 0.0996\n",
            "[S2] Step 1500 loss: 0.0973\n",
            "[S2] Step 1600 loss: 0.0993\n",
            "[S2] Step 1700 loss: 0.0993\n",
            "[S2] Step 1800 loss: 0.1009\n",
            "[S2] Step 1900 loss: 0.1004\n",
            "[S2] Step 2000 loss: 0.1002\n",
            "[S2] Step 2100 loss: 0.0985\n",
            "[S2] Step 2200 loss: 0.0996\n",
            "[S2] Step 2300 loss: 0.0990\n",
            "[S2] Step 2400 loss: 0.1000\n",
            "[S2] Step 2500 loss: 0.0995\n",
            "[S2] Step 2600 loss: 0.0973\n",
            "[S2] Step 2700 loss: 0.0920\n",
            "[S2] Step 2800 loss: 0.0996\n",
            "[S2] Step 2900 loss: 0.0997\n",
            "[S2] Step 3000 loss: 0.1006\n",
            "[S2] Step 3100 loss: 0.0985\n",
            "[S2] Step 3200 loss: 0.0989\n",
            "[S2] Step 3300 loss: 0.0994\n",
            "[S2] Step 3400 loss: 0.0982\n",
            "✅ Stage 2 저장 완료: /content/stage2_contrastive_lora8bit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, df): self.ids, self.seqs = df[\"ID\"].tolist(), df[\"seq\"].tolist()\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i): return {\"ID\": self.ids[i], \"seq\": self.seqs[i]}\n",
        "\n",
        "def collate_infer(batch, tok=tokenizer, max_len=1024):\n",
        "    ids  = [b[\"ID\"] for b in batch]; seqs = [b[\"seq\"] for b in batch]\n",
        "    enc = tok.batch_encode_plus(seqs, padding=\"longest\", truncation=True,\n",
        "                                max_length=max_len, return_tensors=\"pt\")\n",
        "    return {\"ids\": ids, \"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}\n",
        "\n",
        "infer_loader = DataLoader(SeqDataset(test_df), batch_size=4, shuffle=False, collate_fn=collate_infer)\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_embeddings(model, loader):\n",
        "    all_ids, embs = [], []\n",
        "    for batch in tqdm(loader, desc=\"Extract\"):\n",
        "        ids = batch[\"ids\"]\n",
        "        input_ids, attn = batch[\"input_ids\"].to(DEVICE), batch[\"attention_mask\"].to(DEVICE)\n",
        "        with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "            out = model(input_ids, attention_mask=attn, output_hidden_states=True)\n",
        "            hs = out.hidden_states[-1]; mask = attn.unsqueeze(-1)\n",
        "            emb = (hs*mask).sum(1)/mask.sum(1).clamp(min=1)\n",
        "        all_ids.extend(ids); embs.append(emb.cpu())\n",
        "        del out, emb; torch.cuda.empty_cache()\n",
        "    return all_ids, torch.vstack(embs)\n",
        "\n",
        "model.eval()\n",
        "ids, emb_tensor = extract_embeddings(model, infer_loader)\n",
        "H = emb_tensor.shape[1]\n",
        "emb_df = pd.DataFrame(emb_tensor.numpy(), columns=[f\"emb_{i:04d}\" for i in range(H)])\n",
        "submission = pd.concat([sub_df[[\"ID\"]], emb_df], axis=1)\n",
        "out_path = \"/content/submission_stage2_lora8bit.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"✅ 제출 파일 생성 완료:\", out_path)\n",
        "submission.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "32v3ZKbteftf",
        "outputId": "3efa6c06-cc22-442d-88c0-bf75b6c6a9cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extract: 100%|██████████| 3428/3428 [13:47<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 제출 파일 생성 완료: /content/submission_stage2_lora8bit.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ID  emb_0000  emb_0001  emb_0002  emb_0003  emb_0004  emb_0005  \\\n",
              "0  TEST_000000  0.139317  0.103059  0.046083 -0.075317 -0.019991  0.040967   \n",
              "1  TEST_000001 -0.012288  0.080854 -0.135634  0.053765 -0.184574  0.244436   \n",
              "2  TEST_000002  0.175488  0.049647 -0.032940 -0.117441  0.001211  0.147004   \n",
              "3  TEST_000003  0.099347  0.076956  0.012087 -0.184017 -0.111378  0.236539   \n",
              "4  TEST_000004 -0.018580  0.064592 -0.106436 -0.075716 -0.026625  0.093453   \n",
              "\n",
              "   emb_0006  emb_0007  emb_0008  ...  emb_1014  emb_1015  emb_1016  emb_1017  \\\n",
              "0 -0.138024 -0.802797  0.184288  ...  0.150061  0.111468  0.070707 -0.075534   \n",
              "1  0.075660 -0.239170  0.157061  ... -0.050305  0.035269 -0.126393 -0.132549   \n",
              "2 -0.281818 -0.675481  0.139306  ...  0.051925 -0.037222  0.080485 -0.000920   \n",
              "3 -0.221122 -0.803930  0.049654  ...  0.120032  0.098271  0.132732 -0.083911   \n",
              "4 -0.300043 -0.765590  0.108570  ...  0.083171 -0.020589 -0.038171 -0.036910   \n",
              "\n",
              "   emb_1018  emb_1019  emb_1020  emb_1021  emb_1022  emb_1023  \n",
              "0 -0.110732  0.017242  0.158625 -0.094519  0.248387  0.101039  \n",
              "1 -0.168340  0.060516 -0.060063 -0.189517  0.623045 -0.149989  \n",
              "2 -0.097887  0.085249  0.187503  0.043505  0.153992  0.066804  \n",
              "3 -0.097768  0.037088  0.159692  0.018994  0.142375 -0.004306  \n",
              "4 -0.012016  0.075029  0.050192 -0.101168  0.129030  0.013768  \n",
              "\n",
              "[5 rows x 1025 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0bf3f8b-fcc5-4086-966a-24636e79bb7b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>emb_0000</th>\n",
              "      <th>emb_0001</th>\n",
              "      <th>emb_0002</th>\n",
              "      <th>emb_0003</th>\n",
              "      <th>emb_0004</th>\n",
              "      <th>emb_0005</th>\n",
              "      <th>emb_0006</th>\n",
              "      <th>emb_0007</th>\n",
              "      <th>emb_0008</th>\n",
              "      <th>...</th>\n",
              "      <th>emb_1014</th>\n",
              "      <th>emb_1015</th>\n",
              "      <th>emb_1016</th>\n",
              "      <th>emb_1017</th>\n",
              "      <th>emb_1018</th>\n",
              "      <th>emb_1019</th>\n",
              "      <th>emb_1020</th>\n",
              "      <th>emb_1021</th>\n",
              "      <th>emb_1022</th>\n",
              "      <th>emb_1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>0.139317</td>\n",
              "      <td>0.103059</td>\n",
              "      <td>0.046083</td>\n",
              "      <td>-0.075317</td>\n",
              "      <td>-0.019991</td>\n",
              "      <td>0.040967</td>\n",
              "      <td>-0.138024</td>\n",
              "      <td>-0.802797</td>\n",
              "      <td>0.184288</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150061</td>\n",
              "      <td>0.111468</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>-0.075534</td>\n",
              "      <td>-0.110732</td>\n",
              "      <td>0.017242</td>\n",
              "      <td>0.158625</td>\n",
              "      <td>-0.094519</td>\n",
              "      <td>0.248387</td>\n",
              "      <td>0.101039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>-0.012288</td>\n",
              "      <td>0.080854</td>\n",
              "      <td>-0.135634</td>\n",
              "      <td>0.053765</td>\n",
              "      <td>-0.184574</td>\n",
              "      <td>0.244436</td>\n",
              "      <td>0.075660</td>\n",
              "      <td>-0.239170</td>\n",
              "      <td>0.157061</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.050305</td>\n",
              "      <td>0.035269</td>\n",
              "      <td>-0.126393</td>\n",
              "      <td>-0.132549</td>\n",
              "      <td>-0.168340</td>\n",
              "      <td>0.060516</td>\n",
              "      <td>-0.060063</td>\n",
              "      <td>-0.189517</td>\n",
              "      <td>0.623045</td>\n",
              "      <td>-0.149989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.175488</td>\n",
              "      <td>0.049647</td>\n",
              "      <td>-0.032940</td>\n",
              "      <td>-0.117441</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.147004</td>\n",
              "      <td>-0.281818</td>\n",
              "      <td>-0.675481</td>\n",
              "      <td>0.139306</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051925</td>\n",
              "      <td>-0.037222</td>\n",
              "      <td>0.080485</td>\n",
              "      <td>-0.000920</td>\n",
              "      <td>-0.097887</td>\n",
              "      <td>0.085249</td>\n",
              "      <td>0.187503</td>\n",
              "      <td>0.043505</td>\n",
              "      <td>0.153992</td>\n",
              "      <td>0.066804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.099347</td>\n",
              "      <td>0.076956</td>\n",
              "      <td>0.012087</td>\n",
              "      <td>-0.184017</td>\n",
              "      <td>-0.111378</td>\n",
              "      <td>0.236539</td>\n",
              "      <td>-0.221122</td>\n",
              "      <td>-0.803930</td>\n",
              "      <td>0.049654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.120032</td>\n",
              "      <td>0.098271</td>\n",
              "      <td>0.132732</td>\n",
              "      <td>-0.083911</td>\n",
              "      <td>-0.097768</td>\n",
              "      <td>0.037088</td>\n",
              "      <td>0.159692</td>\n",
              "      <td>0.018994</td>\n",
              "      <td>0.142375</td>\n",
              "      <td>-0.004306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>-0.018580</td>\n",
              "      <td>0.064592</td>\n",
              "      <td>-0.106436</td>\n",
              "      <td>-0.075716</td>\n",
              "      <td>-0.026625</td>\n",
              "      <td>0.093453</td>\n",
              "      <td>-0.300043</td>\n",
              "      <td>-0.765590</td>\n",
              "      <td>0.108570</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083171</td>\n",
              "      <td>-0.020589</td>\n",
              "      <td>-0.038171</td>\n",
              "      <td>-0.036910</td>\n",
              "      <td>-0.012016</td>\n",
              "      <td>0.075029</td>\n",
              "      <td>0.050192</td>\n",
              "      <td>-0.101168</td>\n",
              "      <td>0.129030</td>\n",
              "      <td>0.013768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1025 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0bf3f8b-fcc5-4086-966a-24636e79bb7b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0bf3f8b-fcc5-4086-966a-24636e79bb7b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0bf3f8b-fcc5-4086-966a-24636e79bb7b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-59237ce8-600a-41d3-b043-1ced17868413\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59237ce8-600a-41d3-b043-1ced17868413')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-59237ce8-600a-41d3-b043-1ced17868413 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "submission"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/submission_stage2_lora8bit.csv\")\n"
      ],
      "metadata": {
        "id": "rBZs2jM76noN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#  제2회 MAI 대회 최종 제출 Inference 파이프라인\n",
        "#  Author: Hanbin (GPT-5 assisted)\n",
        "#  Strategy: Weighted last4 mean + Multi-crop + Reverse complement + Whitening\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 1️⃣ 환경 설정\n",
        "# ---------------------------------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "print(f\"✅ Device: {DEVICE}\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 2️⃣ 데이터 로드\n",
        "# ---------------------------------------------------\n",
        "data_path = \"/content/open\"\n",
        "test_df = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
        "sub_df  = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
        "print(\"✅ Loaded:\", test_df.shape)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 3️⃣ Reverse Complement 생성 함수\n",
        "# ---------------------------------------------------\n",
        "def reverse_complement(seq: str) -> str:\n",
        "    tr = str.maketrans(\"ACGT\", \"TGCA\")\n",
        "    return seq.translate(tr)[::-1]\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 4️⃣ 모델 및 토크나이저 로드\n",
        "# ---------------------------------------------------\n",
        "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = model.to(DEVICE).eval()\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"✅ Model loaded.\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 5️⃣ Multi-crop + Weighted layer embedding 함수\n",
        "# ---------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def get_seq_embedding(seq, model, tokenizer, n_views=3, max_len=512):\n",
        "    \"\"\"랜덤 crop + reverse complement 평균 포함\"\"\"\n",
        "    embs = []\n",
        "    seqs = [seq, reverse_complement(seq)]  # 원본 + 역상보\n",
        "    layer_weights = torch.tensor([0.1, 0.2, 0.3, 0.4]).to(DEVICE)\n",
        "\n",
        "    for s in seqs:\n",
        "        for _ in range(n_views):\n",
        "            offset = np.random.randint(0, max(1, len(s) - max_len))\n",
        "            sub_seq = s[offset:offset + max_len]\n",
        "            tok = tokenizer(\n",
        "                sub_seq, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_len\n",
        "            ).to(DEVICE)\n",
        "            out = model(**tok, output_hidden_states=True)\n",
        "            hs = torch.stack(out.hidden_states[-4:], dim=0)  # (4, B, L, H)\n",
        "            weighted = (hs * layer_weights.view(4,1,1,1)).sum(0)\n",
        "            mask = tok[\"attention_mask\"].unsqueeze(-1)\n",
        "            emb = (weighted * mask).sum(1) / mask.sum(1).clamp(min=1)\n",
        "            embs.append(emb.cpu())\n",
        "\n",
        "    return torch.stack(embs).mean(0)  # 모든 view 평균\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 6️⃣ 전체 배치 임베딩 추출\n",
        "# ---------------------------------------------------\n",
        "all_ids = []\n",
        "all_embs = []\n",
        "\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    emb = get_seq_embedding(row[\"seq\"], model, tokenizer, n_views=3, max_len=512)\n",
        "    all_ids.append(row[\"ID\"])\n",
        "    all_embs.append(emb)\n",
        "\n",
        "emb_tensor = torch.vstack(all_embs)\n",
        "print(\"✅ Embedding tensor shape:\", emb_tensor.shape)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 7️⃣ LayerNorm + PCA Whitening\n",
        "# ---------------------------------------------------\n",
        "from torch.nn.functional import layer_norm\n",
        "\n",
        "emb_normed = layer_norm(emb_tensor, emb_tensor.shape[1:])\n",
        "emb_np = emb_normed.numpy()\n",
        "\n",
        "# PCA Whitening (512차원 축소 + 정규화)\n",
        "pca = PCA(n_components=min(512, emb_np.shape[1]), whiten=True, random_state=SEED)\n",
        "emb_pca = pca.fit_transform(emb_np)\n",
        "\n",
        "# 다시 정규화\n",
        "emb_final = emb_pca / np.linalg.norm(emb_pca, axis=1, keepdims=True)\n",
        "print(\"✅ Final embedding shape:\", emb_final.shape)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 8️⃣ 제출 파일 생성\n",
        "# ---------------------------------------------------\n",
        "emb_cols = [f\"emb_{i:04d}\" for i in range(emb_final.shape[1])]\n",
        "emb_df = pd.DataFrame(emb_final, columns=emb_cols)\n",
        "submission = pd.concat([pd.Series(all_ids, name=\"ID\"), emb_df], axis=1)\n",
        "\n",
        "out_path = \"/content/submission_final_gLM.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"✅ Saved submission file:\", out_path)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 9️⃣ 로컬 다운로드\n",
        "# ---------------------------------------------------\n",
        "from google.colab import files\n",
        "files.download(out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "MvR7lwZZ782F",
        "outputId": "3722e709-b1d3-4946-efaf-c2b1f0249772"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Device: cuda\n",
            "✅ Loaded: (13711, 2)\n",
            "✅ Model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13711/13711 [1:02:22<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embedding tensor shape: torch.Size([13711, 1024])\n",
            "✅ Final embedding shape: (13711, 512)\n",
            "✅ Saved submission file: /content/submission_final_gLM.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ebcb6b40-2174-47b4-a356-ff291658cd2e\", \"submission_final_gLM.csv\", 86393823)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#  제2회 MAI 대회 최종 제출 Inference (Stage2 LoRA 우선)\n",
        "#  Strategy: Weighted last4 + Multi-crop + Reverse complement + PCA Whitening + LayerNorm\n",
        "# ============================================\n",
        "\n",
        "import os, math, numpy as np, pandas as pd, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\n",
        "\n",
        "# (옵션) PEFT 로더: LoRA 체크포인트가 있을 때 사용\n",
        "try:\n",
        "    from peft import AutoPeftModelForMaskedLM\n",
        "    HAS_PEFT = True\n",
        "except Exception:\n",
        "    HAS_PEFT = False\n",
        "\n",
        "# -----------------------------\n",
        "# 0) 환경설정\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"✅ Device:\", DEVICE)\n",
        "\n",
        "# 데이터 경로\n",
        "data_path = \"/content/open\"  # test.csv, sample_submission.csv가 있는 폴더\n",
        "test_df = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
        "sub_df  = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
        "print(\"✅ Loaded:\", test_df.shape, sub_df.shape)\n",
        "\n",
        "# 모델/체크포인트 경로\n",
        "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
        "STAGE2_CKPT = \"/content/stage2_contrastive_lora8bit\"  # 이전 파인튜닝에서 저장한 경로(없으면 자동 폴백)\n",
        "\n",
        "# Inference 하이퍼파라미터\n",
        "N_VIEWS   = 3      # multi-crop views per strand (원본/역상보 각각)\n",
        "MAX_LEN   = 512    # crop 길이 (제출은 1024여도 OK; 여기선 안정성과 속도 균형)\n",
        "N_PCA     = 512    # 최종 차원 (<= 2048)\n",
        "\n",
        "# Layer 가중치 (마지막 4개 레이어)\n",
        "LAYER_WEIGHTS = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
        "\n",
        "# -----------------------------\n",
        "# 1) reverse complement\n",
        "# -----------------------------\n",
        "def reverse_complement(seq: str) -> str:\n",
        "    tr = str.maketrans(\"ACGT\", \"TGCA\")\n",
        "    return seq.translate(tr)[::-1]\n",
        "\n",
        "# -----------------------------\n",
        "# 2) 모델/토크나이저 로드 (Stage2 우선, 폴백은 베이스)\n",
        "# -----------------------------\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)  # 8bit 로 VRAM 절약 (원한다면 FP16로 바꿔도 됨)\n",
        "\n",
        "if os.path.isdir(STAGE2_CKPT) and HAS_PEFT:\n",
        "    print(\"✅ Loading Stage2 LoRA checkpoint:\", STAGE2_CKPT)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(STAGE2_CKPT, trust_remote_code=True)\n",
        "    model = AutoPeftModelForMaskedLM.from_pretrained(\n",
        "        STAGE2_CKPT, trust_remote_code=True,\n",
        "        quantization_config=bnb_config, device_map={\"\": 0}\n",
        "    )\n",
        "else:\n",
        "    # 폴백: 베이스 모델 사용\n",
        "    print(\"⚠️ Stage2 ckpt not found or PEFT unavailable. Falling back to base model.\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\n",
        "        MODEL_ID, trust_remote_code=True,\n",
        "        quantization_config=bnb_config, device_map={\"\": 0}\n",
        "    )\n",
        "\n",
        "model.eval()\n",
        "model.config.use_cache = False\n",
        "LAYER_WEIGHTS = LAYER_WEIGHTS.to(model.device)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) 시퀀스 임베딩 함수\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def embed_one_sequence(seq: str, n_views=N_VIEWS, max_len=MAX_LEN):\n",
        "    \"\"\"원본 + 역상보에 대해 랜덤 crop n_views씩 -> 총 2*n_views view 평균\"\"\"\n",
        "    embs = []\n",
        "    strands = [seq, reverse_complement(seq)]\n",
        "    for s in strands:\n",
        "        L = len(s)\n",
        "        for _ in range(n_views):\n",
        "            if L <= max_len:\n",
        "                sub_seq = s\n",
        "            else:\n",
        "                start = np.random.randint(0, L - max_len + 1)\n",
        "                sub_seq = s[start:start + max_len]\n",
        "\n",
        "            tok = tokenizer(\n",
        "                sub_seq, return_tensors=\"pt\",\n",
        "                truncation=True, padding=True, max_length=max_len\n",
        "            ).to(model.device)\n",
        "\n",
        "            out = model(**tok, output_hidden_states=True)\n",
        "            hs  = torch.stack(out.hidden_states[-4:], dim=0)              # (4, B, L, H)\n",
        "            w   = (hs * LAYER_WEIGHTS.view(4,1,1,1)).sum(0)               # (B, L, H) weighted sum\n",
        "            mask = tok[\"attention_mask\"].unsqueeze(-1)                    # (B, L, 1)\n",
        "            emb  = (w * mask).sum(1) / mask.sum(1).clamp(min=1)           # (B, H)\n",
        "            embs.append(emb.squeeze(0).cpu())\n",
        "\n",
        "    return torch.stack(embs).mean(0)  # (H,)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) 전체 추론 (단순 루프; 안정성 우선)\n",
        "#    * 더 빠르게 하려면 배치화 가능하나 multi-crop/RC 동기화가 복잡해져서 여기선 루프 권장\n",
        "# -----------------------------\n",
        "all_ids, all_embs = [], []\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Embedding\"):\n",
        "    emb = embed_one_sequence(row[\"seq\"])\n",
        "    all_ids.append(row[\"ID\"])\n",
        "    all_embs.append(emb)\n",
        "\n",
        "emb_tensor = torch.vstack(all_embs)           # (N, H)\n",
        "print(\"✅ Raw embedding shape:\", emb_tensor.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) 정규화 + Whitening (PCA)\n",
        "# -----------------------------\n",
        "from torch.nn.functional import layer_norm\n",
        "emb_normed = layer_norm(emb_tensor, emb_tensor.shape[1:])   # (N, H)\n",
        "emb_np = emb_normed.numpy()\n",
        "\n",
        "# PCA 차원 축소 + 화이트닝\n",
        "n_comp = min(N_PCA, emb_np.shape[1])\n",
        "pca = PCA(n_components=n_comp, whiten=True, random_state=SEED)\n",
        "emb_pca = pca.fit_transform(emb_np)                          # (N, n_comp)\n",
        "\n",
        "# L2 normalize (cosine 안정화)\n",
        "emb_final = emb_pca / (np.linalg.norm(emb_pca, axis=1, keepdims=True) + 1e-9)\n",
        "print(\"✅ Final embedding shape:\", emb_final.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) 제출 파일 생성\n",
        "# -----------------------------\n",
        "assert emb_final.shape[1] <= 2048, \"임베딩 차원 2048 초과 금지!\"\n",
        "emb_cols = [f\"emb_{i:04d}\" for i in range(emb_final.shape[1])]\n",
        "emb_df = pd.DataFrame(emb_final, columns=emb_cols)\n",
        "submission = pd.concat([pd.Series(all_ids, name=\"ID\"), emb_df], axis=1)\n",
        "\n",
        "out_path = \"/content/submission_final_stage2_or_base.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"🎯 Saved:\", out_path)\n",
        "\n",
        "# (선택) 바로 다운로드\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(out_path)\n",
        "except Exception as e:\n",
        "    print(\"ℹ️ Colab files.download 실패 시, 왼쪽 Files 패널에서 직접 다운로드하세요.\")\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "id": "mmKuDY9X8AKN",
        "outputId": "14ee7738-3d00-44b3-c31a-955172fdb44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3091077263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# (옵션) PEFT 로더: LoRA 체크포인트가 있을 때 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoPeftModelForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mHAS_PEFT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.17.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from .peft_model import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_cache_to_layer_device_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloftq_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplace_lora_weights_loftq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .other import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mINCLUDE_LINEAR_LAYERS_SHORTHAND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorage_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_auto_gptq_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_gptqmodel_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mverify_tp_plan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling_flash_attention_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_import_flash_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mid_tensor_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_d_fine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDFineForObjectDetectionLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_d_fine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .loss_for_object_detection import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbox_iou\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_keras.src.optimizers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras.src.optimizers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_tf_keras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tf_keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/_tf_keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/_tf_keras/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0melu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexponential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/activations/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/common/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutocastScope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKerasVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_autocast_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/common/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandardize_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mBOOL_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"bool\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}